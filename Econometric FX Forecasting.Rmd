---
title: "Data Mining Techniques Applied to Econometric Panel Data for FX Forecasting"
author: "Charles Naylor"
output:
  pdf_document: 
    toc: true
  html_notebook: default
---

# Introduction

## Summary of Currency Markets

Macroeconomic forecasting is generally considered to be one of the hardest challenges in finance. In comparison with forecasts on stocks or bonds, currency (a.k.a "foreign exchange", or FX) price movements react more purely to the flows of global trade and geopolitical risk, because they have no intrinsic value. A currency's worth is quoted only relative to other currencies.

### The Carry Trade

That said, there is still a basis for investing in currencies in that any holding will earn interest, just like a savings account at a bank. In the USA, the 3-month interest rate is about 2.3%. In Turkey, it's about 17%. Other things being equal, an investor could borrow money in American Dollars, then lend it out in Turkish Lira, and earn the difference between these rates for a nearly 15% annual return. This is called the **carry trade**. 

The carry trade is not riskless because the spot rate, i.e. the number of USD that a Turkish Lira can buy, is not fixed over this period. In fact, undergraduate economics classes still teach that any profit possible from this trade will be neatly ironed out due to spot price movements, a phenomenon known as **Covered Interest Rate Parity**. The persistance of the carry trade in the face of theory is a reminder that economic models elide substantial frictions experienced in the real world.

### Drivers of Returns

In FX, that interest rate differential, the "carry", is relatively stable but subject to punctuated equilibrium as new data comes to light. In developed markets, this data primarily consists of central bank rate decisions and the economic news that might affect those decisions. Price movements, however, are the deterministic result of countless iterative, interacting agents. While we may know many of these agents’ motives, it is not possible to aggregate their behavior with any accuracy, because the actions of each agent are affected by those of all of the other agents, and small measurement errors compound. For example, it is impossible to tell the periodicity of market data without context. A day’s worth of price movements at 5-minute increments looks the same as a year’s worth of daily movements. The asset price measures the result of a chaotic, nonlinear dynamical system: a scale-free network`<insert reference to Barabasi>`.

## Motivation

Speculators in currencies try to pocket the carry while avoiding the risk of major movements in currency prices. The strategy has been likened to picking up nickels in front of a steamroller. Thus, in spite of the difficulties inherent in forecasting currency movements, a *successful* speculator must have some idea of what those movements will look like in the future.

There are many possible methods available to create these forecasts. Professionally, econometric panel data (i.e. the same set of economic measurements repeated for multiple countries) has been plugged into a Kalman Filter in order to capture the evolving relationship between indicators and their currencies, while also recognizing that one currency's movements will affect all other currencies. The Kalman filter has the advantage of having a closed-form solution and being well-adapted to testing in systems in which one expects to add new data regularly. In its most basic form, the disadvantage is that a Kalman filter requires careful tuning of the relationship between its output variables, and of the covariance of evolution of its factor weightings.

Prior work has been done on the same data set as a case study of generative Bayesian forecasting techniques`<insert GP reference>`. The study applied a Gaussian Process Regression to panel data for 11 of the most traded world currencies. The result was a well-specified and validated forecasting model whose error term was far larger than its signal. This jibed with the results seen professionally using a Kalman Filter, but thanks to posterior predictive checking the flaws are glaringly obvious. Can this, or a similar forecast, be improved?

Work is being done at the University of Maryland into the application of neural network reservoirs to chaotic systems.`<insert Pathak reference>` After surveying some simpler machine learning techniques, this project will apply these techniques to the residuals of the Gaussian Process regression, or in combination with some other regression on the panel data.

## Goals

  * Run the panel data through a set of standard data mining techniques to provide a baseline.
  * Apply reservoir computing techniques to existing or new regressions.

# The Data

Data consists of the weekly currency returns, plus a raft of weekly economic data.

```{r load_data, include=FALSE}
packages <- c("tidyverse", "scales", "lubridate", "ggthemes", "randomForest", "cluster", "e1071", "abind")
lapply(packages, function(X){suppressPackageStartupMessages(library(X, character.only=T))})

load('data/scaled_data.rData')

#Put 3d exog matrix into tidy format
exogs %>%
  as_tibble() %>%
  rownames_to_column("Date") %>%
  mutate(Date=as.Date(Date)) %>%
  gather(asset_exog,value,-Date) %>%
  separate(asset_exog, c("Asset","Exog"), sep="\\.") ->
  exogs

#Set standard color palettes
asset_colors <- c('#a6cee3','#1f78b4','#b2df8a','#33a02c',
                 '#fb9a99','#e31a1c','#fdbf6f','#ff7f00',
                 '#cab2d6','#6a3d9a','#ffff99') # http://colorbrewer2.org/?type=qualitative&scheme=Paired&n=11
names(asset_colors) <- colnames(endo)[-1]

exog_colors <- c('#1b9e77','#d95f02','#7570b3','#e7298a',
                 '#66a61e','#e6ab02') #http://colorbrewer2.org/?type=qualitative&scheme=Dark2&n=6
names(exog_colors) <- exogs %>% pull(Exog) %>% unique()
```

## Y variables

The endogenous, 'Y' variable comprises weekly currency returns on 11 currencies for a ten-year period starting in 2007 and ending in 2017.

Weekly currency returns consist of the change in spot rate against the US dollar, plus the carry, defined as the (time-adjusted) difference in 1 month forward rates between the local currency and the USD. Without going into detail, the forward rates are a reasonable proxy for what a professional speculator might expect to earn by placing trades using currency derivatives.

It's important to note that our endogenous variable is *multivariate*. We cannot assume that different currencies have returns that are independent of one another.

```{r graph_endos, echo=FALSE, fig.height=5, fig.width=8.5}
endo %>%
  gather(Asset,value,-Date) %>%
  group_by(Asset) %>%
  mutate(value = cumsum(value)) %>% #Sum b/c they are diffed logs
  ggplot(aes(x=Date,y=value, col=Asset)) +  theme_gray() +
    scale_color_manual(values=asset_colors) +
    scale_y_continuous(labels=scales::percent) +
    geom_hline(aes(yintercept=0), col="darkgrey") + 
    geom_line() +
    ggtitle("Cumulative Returns vs. USD") + ylab("") + xlab("")
```

## X variables

The exogenous, 'X' variables consist of weekly data for the following:

| Factor           | Description                                 |
|------------------|---------------------------------------------|
| Equity_d8W       | The 8 week change in the local equity index |
| Spot_d8W         | The 8 week change in spot rates             |
| SwapRate2Y       | The 2Y Swap Rate                            |
| TwoYear_d8W      | The 8 week change in 2Y Swap rate           |
| YieldCurve       | The spread between 10Y and 2Y Swaps         |

There is clear room for improvement if we were to fit a model that permitted multiple time scales, such as a MIDAS regression`<insert midas ref>`. It would also be usual to include some sort of measure of liquidity and credit conditions.

These variables have been scaled and normalized.

```{r graph_exogs, echo=FALSE, fig.height=5.5, fig.width=4.5}
exogs %>% 
  filter(!(Exog=="constant")) %>%
  ggplot(aes(x=Date,y=value)) +
  theme_gray() +
  facet_grid(Asset~Exog) +
  geom_hline(yintercept=0, color="darkgrey") +
  geom_line() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  ylab("") + xlab("") + ggtitle("Exogenous Variables")
```

Note that Carry is not a stochastic variable. It is, however, a rate, and impacts asset returns consistently at that rate. It's also worth noting the degree to which rate differentials have vanished since the Credit Crisis of 2008.

# Models

## Classification

### Introduction: Regime Switching Models

There has been a substantial amount of work done in economic forecasting in which time periods are split up into *regimes*, periods in which markets are expected to behave similarly given similar data. For example, the markets in the run up to the bursting of the tech bubble in 2000, or the housing credit bubble in 2008, behaved substantially differently to the markets just after those bubbles. The Kalman Filter and Gaussian Process regression models attempt to account for changing reactions to market conditions by fitting a continuously changing regression to those conditions. 

One alternative is a **Regime Switching Model**. Split the time periods into several regimes, then run a separate regression for each regime. The process is complicated by the fact what it can be extremely difficult to tell at any given moment in which regime markets were operating at any given time, even in hindsight.

One can attempt to determine these regimes by running various classifiers and clustering algorithms against the exogenous and endogenous data.

### Dissimilarity Matrixes

First it is necessary to compute a dissimilarity matrix between periods. This calculates the distance between all of the values we have for each week.

While fitting a global set of regimes, will give a more comprehensible overview of the techniques, one set of regimes per country will be more useful for forecasting, as the primary interest is in what happens when there are differences between countries.

#### Global Regimes

Assume the market reactions to economic conditions are the same across all countries.

```{r global_diss, cache=TRUE}
# Organize data into per-row observation matrix
endo %>%
  gather(Asset, value, -Date) %>%
  mutate(Exog="Endo") %>%
  bind_rows(exogs) %>%
  filter(Date >=as.Date("2008-01-04")) %>% #endo has more data than exog at the beginning
  filter(Date <=as.Date("2017-08-18")) %>% #exog has more data than endo at the end
  unite(asset_exog, c("Asset", "Exog")) %>%
  spread(asset_exog, value) ->
  all_obs_matrix

all_obs_matrix %>% 
  select(-Date) %>%
  daisy(metric="euclidean") ->
  global_dissimilarity
```


#### Principal Component Analysis

It will be hard to see what any of these clusters looks like, as the feature space is high-dimensional. First, groupings of weeks were examined using the first two principal components. These might form the basis of later visualizations.

```{r pr_comp, fig.height=2, fig.width=3}
all_obs_matrix %>% 
  select(-Date) %>% 
  prcomp() -> 
  global_pca

#Plot
(global_pca$x %*% global_pca$rotation[,1:2]) %>%
  as_tibble() %>%
  bind_cols(all_obs_matrix %>% 
                transmute(Year=decimal_date(Date))) %>%
  ggplot(aes(x=PC1,y=PC2, col=Year)) +
  theme_bw() +
  scale_color_gradient(low="red",high="blue")+
  geom_point(alpha=0.25) + 
  ggtitle("First 2 Principal Components")
```
There is not much evidence of clear regime clusters here.

#### Per-Country Regimes

How similar are the regimes between countries? There may be an identification issue as there's no guarantee that, e.g. regime 1 in one country will be encoded as regime 1 in another, even if they refer to similar underlying clusters.

```{r per_asset_diss, include=FALSE}
endo %>%
  gather(Asset, value, -Date) %>%
  mutate(Exog="Endo") %>%
  bind_rows(exogs) %>%
  filter(Date >=as.Date("2008-01-04")) %>% #endo has more data than exog at the beginning
  filter(Date <=as.Date("2017-08-18")) %>% #exog has more data than endo at the end
  spread(Exog, value) %>% 
  nest(-Asset, .key="exogs") %>%
  mutate(diss=map(exogs, ~list(Date=.$Date,diss=daisy(.[,-1], metric="euclidean")))) %>%
  select(-exogs) ->
  asset_dissimilarity #we may use these later for weighted OLS
```

### Hierarchical Clustering

In hierarchical clustering, the observations are grouped by similarity into progressively larger sections, creating a branching set of categories. Given that the observer should expect relatively few broad regimes of behavior in total, it will be necessary to cut these branches early. Probably there should only be 2 or 3 regimes in total, but for the purposes of validation, the cut was made at 5.

The identified cluster for each week can be seen below:

```{r hclust, fig.height=4, fig.width=8, cache=TRUE}
global_hclust <- hclust(global_dissimilarity, method='complete')

all_obs_matrix %>% 
  select(Date) %>%
  mutate(Regime=factor(cutree(global_hclust, k=5)),
         Year=factor(year(Date)),
         Week=week(Date)) %>%
  ggplot(aes(x=Week, y=0, fill=Regime)) +
    theme_pander() +
    facet_grid(Year~., switch="y") +
    geom_tile(color="white") + 
    scale_fill_brewer(type="qual",palette="Set1") +
    theme(strip.text.y = element_text(angle=180),
          axis.text.y=element_blank(), 
          axis.ticks.y = element_blank()) + ylab("") +
    ggtitle("Hierarchical Clustering Global Regimes")
```
There are a lot of contiguous blocks, so the technique has clearly identified something.

#### Per-country

How well does the technique work when applied to individual countries?

```{r per_asset_hclust, echo=FALSE}
asset_dissimilarity %>%
  mutate(Regime=map(diss, 
                    ~tibble(Date=.$Date,
                          Regime=hclust(.$diss, method="complete") %>% 
                            cutree(k=3)))
         ) %>%
  select(-diss) %>%
  unnest(Regime) %>% 
  mutate(Regime=factor(Regime),
         Year=factor(year(Date)),
         Week=week(Date),
         Asset=factor(Asset) %>% fct_rev) %>% 
  ggplot(aes(x=Week, y=Asset, fill=Regime)) +
    theme_pander() +
    facet_grid(Year~., switch="y") +
    geom_tile(color="white") + 
    scale_fill_brewer(type="qual",palette="Set1") +
    theme(strip.text.y = element_text(angle=180, size=10),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank()) + ylab("Row Per Asset") +
    ggtitle("Hierarchical Clustering Per-Asset Regimes")
```
There is still quite a lot of commonality between regimes. The least similar currency is the Turkish Lira, on the bottom row. The British Pound (middle row) enters a new regime 3 weeks *before* the vote on exiting the EU, in week 25 of 2016, which is evidence against the validity of this technique.

### K-means Clustering

Unlike hierarchical clustering, K-means clustering looks at the data holistically, and attempts to create groups of points around an arbitrary number of centers. 5 centers were chosen to match the regimes identified earlier.

```{r global_kmean, cache=TRUE}
global_kmeans <- kmeans(all_obs_matrix %>% select(-Date),
                        centers=5)

all_obs_matrix %>% 
  select(Date) %>%
  mutate(Regime=factor(global_kmeans$cluster),
         Year=factor(year(Date)),
         Week=week(Date)) %>%
  ggplot(aes(x=Week, y=0, fill=Regime)) +
    theme_pander() +
    facet_grid(Year~., switch="y") +
    geom_tile(color="white") + 
    scale_fill_brewer(type="qual",palette="Set1") +
    theme(strip.text.y = element_text(angle=180),
          axis.text.y=element_blank(), 
          axis.ticks.y = element_blank()) + ylab("") +
    ggtitle("Hierarchical Clustering Global Regimes")
```

Although the Regime numbers are different, the pattern looks fairly similar to the global regimes made using hierarchical clustering.

# Forecasting

As with most other asset classes, currency returns have fat tails, and the majority of an investor's profit (or loss) is typically made in a small number of periods. Suprise economic news or geopolitical events such as the British vote to leave the EU can cause the markets to reassess appropriate price levels drastically. As a result, a person discretizing returns so that a classifier can be applied directly would be well-advised to distinguish between large moves and smaller ones. In this analysis, discretization of the endogenous variable will be avoided as much as possible.

One must also be careful when applying typical validation techniques, particularly cross-validation, to time series. As the clustering algorithms above clearly demonstrated, one cannot take samples of the data that assume values will be independent of one another across time. The ideal equivalent to leave-one-out cross-validation is simply to run the algorithm every week with the data available for that week, keeping a single period forecast each time. For the purposes of regressions conducted with Support Vector Machines and Random Forests, however, the data will be sliced into training and testing periods midway through the time series.

To make things convenient, move the data out of tidy format, and match up next week's returns to this week's forecasting data:
```{r}
TRAINING_CUTOFF = ymd(20121029)
#stack and combine lagged asset returns with exogenous variables
endo %>%
  mutate_at(vars(-Date),lead) %>% #dplyr parlance has moving endos one week back as a "lead".
  remove_missing() %>%
  gather(Asset, endo, -Date) %>%
  inner_join(exogs, by=c("Date","Asset")) %>% 
  mutate(is_training= Date<TRAINING_CUTOFF) %>%
  spread(Exog, value) ->
  forecast_data
glimpse(forecast_data)
```

### Support Vector Machines

Although Support Vector Machines (SVMs) are traditionally used for classification, the technique can also be applied to regression.

The exogenous variables have already been standardized, so a SVM should fit reasonably well out of the box. As we are forecasting at this point, the endogenous variables (i.e. the asset returns) will need to be lagged by a week. Finally, there is the question of whether all currencies should be expected to behave similarly given the same economic data. For now, it will be assumed this is so.

Try an initial run without tuning:
```{r svm_base, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date,-Asset, -is_training) %>% #a constant carries no information for a support vector machine, I believe
  {svm(as.formula("endo~."), data=., type="eps-regression", kernel="radial")} ->
  svm_base
summary(svm_base)
```
2500 support vectors suggests the regression may not be robust. How well does it forecast returns for the remaining period?

```{r}
forecast_data %>%
  filter(!is_training) %>%
  select(-constant, -Date, -Asset, -is_training) %>%
  {predict(svm_base, newdata = .)} -> 
  svm_base_hat

forecast_data %>%
  filter(!is_training) %>%
  select(endo) %>% 
  bind_cols(svm_base=svm_base_hat) %>%
  summarize(SSR=sum((endo-svm_base)^2)) %>% pull(SSR) %>% print(digits=3)
```
It remains to be seen how good that SSR score is. Practitioners typically have low expectations for goodness of fit and rely on various performance metrics after applying an optimizer to the asset forecasts to determine realistic investment decisions that could be based on the results. Asset optimization is out of scope here, however.

#### Tuning the SVM
The `e1071` package can perform a grid search on available parameters and tune itself, however, the algorithm assumes that cross-validation is an acceptable techniqe. The SVM will have to be tuned manually.

What about a more parsimonious representation? The SVM algorithm permits a regularization parameter, `cost`, which will penalize overly complex models. It would also be sensible to try a polynomial kernel, as asset returns, even normalized, are not expected to be linear with respect to the economic data. Finally, it is likely this regression will be noisy, so $\gamma$ should be set low to reflect the high variance in the data.

```{r svm_occam, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date,-Asset, -is_training) %>% #a constant carries no information for a support vector machine, I believe
  {svm(as.formula("endo~."), data=., type="eps-regression", kernel="radial", cost=1000)} ->
  svm_occam
summary(svm_occam)
```
The cost parameter actually increased the number of support vectors found.

Fit a polynomial kernel:
```{r svm_poly, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date,-Asset, -is_training) %>% #a constant carries no information for a support vector machine, I believe
  {svm(as.formula("endo~."), data=., type="eps-regression", kernel="polynomial", coe0=0, d=3, cost=1000)} ->
  svm_poly
summary(svm_poly)
```
This one failed to converge, so higher orders of polynomial are right out.

Finally, set gamma low:
```{r svm_noisy, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date,-Asset, -is_training) %>% #a constant carries no information for a support vector machine, I believe
  {svm(as.formula("endo~."), data=., type="eps-regression", kernel="radial", cost=1000, gamma=0.001)} ->
  svm_gamma
summary(svm_gamma)
```

Compare the four models, using Mean Squared Error this time:
```{r svm_ssr, cache=TRUE}
list(base=svm_base, simple=svm_occam, poly=svm_poly, gamma=svm_gamma) %>%
  map_dfr(function(model) {
    forecast_data %>%
      filter(!is_training) %>%
      select(-constant, -Date, -Asset, -is_training) %>%
      {predict(model, newdata = .)} %>% 
      bind_cols(hat=.,
                forecast_data %>% 
                  filter(!is_training) %>% 
                  select(endo)) %>%
      summarize(MSE=10000*mean((endo-hat)^2)) %>% 
      select(MSE) 
  }, .id="SVM") %>%
  knitr::kable(digits=3)
```

The model assuming higher noise did the best. Something very strange is going on the the high-cost 'simple' model, which wound up less simple than the base.

It's notable for all of these that the number of support vectors is close to the number of observations (around 90%), and that high penalties are not helping the model find an easier solution. SVMs are probably the wrong approach for this forecasts this noisy.


## Random Forests

Decision Trees (or Regression Trees, in the continuous case) divide observations into sections according to their similarity, and relate those sections to the endogenous variable. They are notoriously unstable, however: a small change in data can result in a completely different model. Random Forests attempt to address this instability by taking random samples of the data and fitting separate sets of Regression Trees to each set, then combining the results. In their raw form, they must be used with caution as the random sampling will assume observations are independent, when in fact they are correlated over time. The `randomForest` function in R can also be set to return a proximity measure, similar to the dissimilarity matrixes calculated above for the classifiers. More on this later.


```{r rf, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date, -Asset, -is_training) %>%
  {randomForest(as.formula("endo~."), data=.)} ->
  rf_base

plot(rf_base)
```

There is not much gain from fitting more than about 25 trees.

As before, there should be relatively few nodes in the tree, reflecting relatively few possible macroeconomic regimes for economies. How well does the Random Forest regression do when `maxnodes` has been set quite low?

```{r rf_fewnodes, cache=TRUE}
forecast_data %>%
  filter(is_training) %>%
  select(-constant, -Date, -Asset, -is_training) %>%
  {randomForest(as.formula("endo~."), data=., maxnodes=20)} ->
  rf_fewnodes

plot(rf_fewnodes)
```

That seems to have done just as well. Update the Mean Squared Error table:
```{r rf_mse}
list(svm_base=svm_base, svm_simple=svm_occam, svm_poly=svm_poly, 
     svm_gamma=svm_gamma, rndForest=rf_base, rf_fewnodes=rf_fewnodes) %>%
  map_dfr(function(model) {
    forecast_data %>%
      filter(!is_training) %>%
      select(-constant, -Date, -Asset, -is_training) %>%
      {predict(model, newdata = .)} %>% 
      bind_cols(hat=.,
                forecast_data %>% 
                  filter(!is_training) %>% 
                  select(endo)) %>%
      summarize(MSE=10000*mean((endo-hat)^2)) %>% 
      select(MSE) 
  }, .id="Model") ->
  MSE
  MSE %>%
  arrange(desc(MSE)) %>%
  knitr::kable(digits=3)
```

Fewer nodes made for a more robust model, but it is only marginally better than the noise-tolerant SVM.

## Analogy Weighting

Jim Savage, currently chief Data Scientist at Lendable, Inc., invented the technique of Analogy Weighting for his PhD thesis. His code is available [here](https://github.com/khakieconomics/Thesis_work). One runs a weighted linear regression on time series data using the proximity matrix from a Random Forest regression for the weights. The intuition is that similar periods in history ought to elicit similar reactions to the data. Dr. Savage used the data to forecast three correlated economic time series. A key advantage of the analogy weighting methodology is that the weights can be set such that the regression is run on each data point without looking into the future. This mimics the Kalman Filter workflow in that an investor need not fit the entire model over again for each new data point. Alternately, one can see an implementation of a Gaussian Process Regression on this same data [here](https://charlesnaylor.github.io/gp_regression/). It was necessary to fit the model over 500 times, which took considerable processing power.

For an additional challenge, this forecast will take into account the differences between countries, while fitting a single set of betas to all. Outcomes will be modeled as arising from a multivariate normal process to mimic the manner in which different countries' currencies affect one another.

```{r proxmat, cache=TRUE}

# Generate the proximity matrix once for each currency, then average
proximate <- function(X) {
    prox_mat <- randomForest(endo~carry+d_carry+eqy_z+spot_z+yc_z, data=X, proximity=TRUE)$proximity
    data.frame(Asset=X$Asset, Date=X$Date, prox_mat, stringsAsFactors = F) %>%
      gather(x_Date,value,-Asset,-Date) %>%
      mutate(x_Date=X$Date[as.integer(gsub("X","",x_Date))]) #make sure the matrix stays lined up properly
}

forecast_data %>%
  select(-constant, -is_training) %>% 
  group_by(Asset) %>%
  do(proximate(.)) %>%
  group_by(Date, x_Date) %>%
  summarize(value=mean(value)) ->
  proximity

proximity %>%
  mutate(value=ifelse(x_Date>=Date,0,value)) %>%
  ggplot(aes(x=x_Date,y=Date,fill=value)) +
  geom_tile() + #I find myself using geom_tile a lot on this project
  scale_fill_gradient(low="white", high="red") +
  coord_flip()+
  ggtitle("Similarity between Dates")
```

Weightings will probably need to be normalized.

### Panel estimation of weighted regressions

The estimation of a multivariate endogenous variable is complex and best expressed probabilistically using a generative model. Properly, fake data should be generated with known parameters, and the model validated by showing that it can recover those parameters. An example of the validation workflow can be found can be found [here](https://charlesnaylor.github.io/gp_regression/doc/Specifying_the_Model-Full_Model.html) for the Gaussian Process regression mentioned earlier.

The model will be fit using Stan, the current state of the art for Bayesian generative models. Stan fits using Hamiltonian Monte Carlo, and compiles directly to C for efficiency's sake, so the code in R will be specified as a long string.

As a final input, the estimated covariance of the various currency returns will be specified directly, for parsimony's sake. A production system would typically estimate the joint volatility using a GARCH model.

```{r}
suppressPackageStartupMessages(library(rstan))
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()) # MCMC techniques involve running multiple chains and ensuring they mixed well, an easily paralellized problem.

weighted_mvn_code <- "
data {
  int<lower=0> A; // # of Assets
  int<lower=0> T; // # of weeks (recall that the linear regression weighting permits testing on t+1 forecasts in a single step)
  int<lower=0> J; // # of exogenous variables
  int<lower=0> T_1; // #start of training period. We need some padding so the regression has enough periods to be useful 
  
  row_vector[A] Y[T]; // asset returns
  matrix[J,A] X[T]; //exogenous panel data
  cov_matrix[A] y_cov[T]; // Covariance of asset returns
  matrix[T,T] proxmat; //proximity matrix for weighting log-likelihood
}
transformed data {
  int<lower=0> TL; //#training period length
  matrix[A,A] L_y_cov[T];
  TL = T - T_1;
  for(t in 1:T) {
    L_y_cov[t] = cholesky_decompose(y_cov[t]); // Cholesky decompositions are more stably positive-definite than straight covariance matrices
  }
}
parameters {
  matrix[TL, J] beta; // Shrunk betas across all assets. Could have been estimated as a state space model
}
model {
  to_vector(beta) ~ normal(0, 1);
  for(t in T_1:(T-1)) { //for each week in the data
      for(i in 1:t) { //for each period prior to that week
        target += proxmat[t,i] * multi_normal_cholesky_lpdf(Y[i]|beta[t-T_1+1]*X[i], L_y_cov[i]);
      }
    }
}
generated quantities {
  //Generate forecasts so that we will have plenty of data to feed the neural net.
  vector[A] y_hat[TL];
  vector[TL] log_lik;
  log_lik[1] = multi_normal_cholesky_lpdf(Y[T_1]|beta[T_1]*X[T_1], L_y_cov[T_1]);
  for(t in (T_1+1):(T-1)) {
    log_lik[t-T_1+1] = multi_normal_cholesky_lpdf(Y[t]|beta[t-T_1+1]*X[t-T_1+1], L_y_cov[t-T_1+1]);
    y_hat[t-T_1+1] = multi_normal_cholesky_rng(to_vector(beta[t-T_1]*X[t-T_1+1]), L_y_cov[t-T_1+1]);
  } 
}
"

weighted_mvn_model <- stan_model(model_name="weighted_mvn", model_code=weighted_mvn_code)
```

Prepare the data in the format Stan expects. This was defined in the `data` section above.

#### $\Sigma_y$

Create exponentially weighted variance-covariance matrix. GARCH would be better, but from prior experience makes little difference to weekly currency returns.
```{r y_cov}
FIRST_DATA_SET <- 52 #Start the forecast after we have at least 52 weeks of data
N1 <- nrow(endo) - 1 - FIRST_DATA_SET
N2 <- 0 # # of extra periods
A <- ncol(endo)-1

y_cov <- array(NA, dim=c(N1+N2, A, A))

lambda <- exp(log(0.5)/52) # 1Y half-life exponential decay
wts <- lambda ^ (seq.int(0, y_offset+N1+N2))
for(n in 1:nrow(y_cov)) {
  ind <- seq.int(FIRST_DATA_SET+n-1) #extra -1 in the index because we are lagging these an extra step
  y_cov[n,,] <- cov.wt(endo[ind,-1],rev(wts[ind]), center=F)$cov  
}
```

Prepare the rest of the data

```{r stan_data}
stan_data <- list(A=length(unique(forecast_data$Asset)),
                  T=length(unique(forecast_data$Date)),
                  J=6,T_1=52,
                  Y=forecast_data %>% 
                    select(Date, Asset, endo) %>% 
                    spread(Asset, endo) %>%
                    select(-Date),
                  X=forecast_data %>% 
                    select(-endo, -is_training, -Date) %>%
                    nest(-Asset) %>%
                    {abind(.$data, along=3, new.names=.$Asset)},
                  y_cov=y_cov[3:504,,],
                  proxmat=proximity %>% 
                    ungroup() %>%
                    spread(x_Date, value) %>%
                    select(-Date) %>% 
                    as.matrix())
```

Run direct maximum likelihood optimization instead of Hamiltonian Monte Carlo due to time constraints.
```{r optim_stan, cache=TRUE}
fit_weighted_mvn <- optimizing(weighted_mvn_model, data=stan_data)
```
Extract parameters
```{r}
matrix(fit_weighted_mvn$par[grep("beta", names(fit_weighted_mvn$par))],
       with(stan_data,T-T_1), stan_data$J, byrow=T, dimnames=list(NULL, unique(exogs$Exog))) %>%
  as.data.frame() %>%
  mutate(Date=endo$Date[stan_data$T_1:(stan_data$T-1)]) %>%
  gather(Exog,value,-Date) ->
  beta

beta %>% 
  ggplot(aes(x=Date, y=value, col=Exog)) + geom_line(alpha=0.6) + ggtitle("Betas over Time") + scale_color_manual(values=exog_colors)
```
The betas could be artificially induced to be more stable, as in a State Space Model. The fact that they are not naturally so, and have such small values, indicates that most of the variation in asset returns cannot be explained by the weighted model.

How well did the model predict returns?

```{r}
matrix(fit_weighted_mvn$par[grep("y_hat", names(fit_weighted_mvn$par))],
       with(stan_data,T-T_1), stan_data$A, byrow=T, dimnames=list(NULL, colnames(endo)[-1])) %>%
  as.data.frame() %>%
  mutate(Date=endo$Date[stan_data$T_1:(stan_data$T-1)]) %>%
  gather(Asset,y_hat,-Date) ->
  y_hat

y_hat %>% 
  drop_na() %>%
  inner_join(endo %>% gather(Asset, value, -Date), by=c("Asset","Date")) %>% 
  summarize(Model="analogy_mvn", MSE=10000*mean((value-y_hat)^2)) %>% 
  bind_rows(MSE) %>%
  arrange(desc(MSE)) ->
  MSE
MSE %>% knitr::kable(digits=3)
```
On its own, the analogy-weighted model is one of the poorest options.

## Taming the Chaos



# References