\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Data Mining Techniques Applied to Econometric Panel Data for FX Forecasting},
            pdfauthor={Charles Naylor},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Data Mining Techniques Applied to Econometric Panel Data for FX
Forecasting}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Charles Naylor}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\section{Introduction}\label{introduction}

\subsection{Summary of Currency
Markets}\label{summary-of-currency-markets}

Macroeconomic forecasting is generally considered to be one of the
hardest challenges in finance. In comparison with forecasts on stocks or
bonds, currency (a.k.a ``foreign exchange'', or FX) price movements
react more purely to the flows of global trade and geopolitical risk,
because they have no intrinsic value. A currency's worth is quoted only
relative to other currencies.

\subsubsection{The Carry Trade}\label{the-carry-trade}

That said, there is still a basis for investing in currencies in that
any holding will earn interest, just like a savings account at a bank.
In the USA, the 3-month interest rate is about 2.3\%. In Turkey, it's
about 17\%. Other things being equal, an investor could borrow money in
American Dollars, then lend it out in Turkish Lira, and earn the
difference between these rates for a nearly 15\% annual return. This is
called the \textbf{carry trade}.

The carry trade is not riskless because the spot rate, i.e.~the number
of USD that a Turkish Lira can buy, is not fixed over this period. In
fact, undergraduate economics classes still teach that any profit
possible from this trade will be neatly ironed out due to spot price
movements, a phenomenon known as \textbf{Covered Interest Rate Parity}.
The persistance of the carry trade in the face of theory is a reminder
that economic models elide substantial frictions experienced in the real
world.

\subsubsection{Drivers of Returns}\label{drivers-of-returns}

In FX, that interest rate differential, the ``carry'', is relatively
stable but subject to punctuated equilibrium as new data comes to light.
In developed markets, this data primarily consists of central bank rate
decisions and the economic news that might affect those decisions. Price
movements, however, are the deterministic result of countless iterative,
interacting agents. While we may know many of these agents' motives, it
is not possible to aggregate their behavior with any accuracy, because
the actions of each agent are affected by those of all of the other
agents, and small measurement errors compound. For example, it is
impossible to tell the periodicity of market data without context. A
day's worth of price movements at 5-minute increments looks the same as
a year's worth of daily movements. The asset price measures the result
of a chaotic, nonlinear dynamical system: a scale-free network (Barabasi
2002).

\subsection{Motivation}\label{motivation}

Speculators in currencies try to pocket the carry while avoiding the
risk of major movements in currency prices. The strategy has been
likened to picking up nickels in front of a steamroller. Thus, in spite
of the difficulties inherent in forecasting currency movements, a
\emph{successful} speculator must have some idea of what those movements
will look like in the future.

There are many possible methods available to create these forecasts.
Professionally, econometric panel data (i.e.~the same set of economic
measurements repeated for multiple countries) has been plugged into a
Kalman Filter in order to capture the evolving relationship between
indicators and their currencies, while also recognizing that one
currency's movements will affect all other currencies. The Kalman filter
has the advantage of having a closed-form solution and being
well-adapted to testing in systems in which one expects to add new data
regularly. In its most basic form, the disadvantage is that a Kalman
filter requires careful tuning of the relationship between its output
variables, and of the covariance of evolution of its factor weightings.

Prior work has been done on the same data set as a case study of
generative Bayesian forecasting techniques (Naylor 2017). The study
applied a Gaussian Process Regression to panel data for 11 of the most
traded world currencies. The result was a well-specified and validated
forecasting model whose error term was far larger than its signal. This
jibed with the results seen professionally using a Kalman Filter, but
thanks to posterior predictive checking the flaws are glaringly obvious.
Can this, or a similar forecast, be improved?

Work is being done at the University of Maryland into the application of
neural network reservoirs to chaotic systems (Pathak et al. 2018). After
surveying some simpler machine learning techniques, this project will
apply these techniques to the residuals of the Gaussian Process
regression, or in combination with some other regression on the panel
data.

\subsection{Goals}\label{goals}

\begin{itemize}
\tightlist
\item
  Run the panel data through a set of standard data mining techniques to
  provide a baseline.
\item
  Apply reservoir computing techniques to existing or new regressions.
\end{itemize}

\section{The Data}\label{the-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}
    \DataTypeTok{message =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{warning =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{include =} \OtherTok{FALSE}
\NormalTok{)}
\NormalTok{packages <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"scales"}\NormalTok{, }\StringTok{"lubridate"}\NormalTok{, }\StringTok{"ggthemes"}\NormalTok{, }\StringTok{"randomForest"}\NormalTok{, }\StringTok{"cluster"}\NormalTok{, }\StringTok{"e1071"}\NormalTok{, }\StringTok{"abind"}\NormalTok{)}
\KeywordTok{lapply}\NormalTok{(packages, }\ControlFlowTok{function}\NormalTok{(X)\{}\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(X, }\DataTypeTok{character.only=}\NormalTok{T))\})}
\end{Highlighting}
\end{Shaded}

Data consists of the weekly currency returns, plus a raft of weekly
economic data.

\subsection{Initial Data Gathering}\label{initial-data-gathering}

While efforts were made to use free data as much as possible, even after
extensive clean-up, much of it was of inadequate quality. Even
proprietary data from Bloomberg's professional API took some work to put
into proper format. Shown below is a digest of some of the tribulations
undergone in putting the economic data in order. The full experience can
be examined in the documentation for (Naylor 2017).

\subsubsection{Swap Rates}\label{swap-rates}

As mentioned before, returns on FX consist of two elements: changes in
the spot rate, i.e.~the exchange rate for physical currency, and the
interest rate differential between the two currencies in the pair. In
this study, all returns were considered versus the USD.

Swap rates are the interest rates offered on short-term debt instruments
known as Swaps, and are often used as a proxy for the interest rate
differential. The ideal source would be the implied 1M forward rate,
which is literally what investors will receive. There were a few options
to check.

Read in raw data from Bloomberg.

Even in supposedly clean and pre-checked data from Bloomberg, there were
clear errors. The Norwegian Krone did not pay -60\% monthly interest in
the year 2000. Turkey, however, did suffer from hyper-inflation in the
early 2000s, and so those high values are probably accurate. The EUR 1M
Swap rate is obviously wrong, too; it's been the same value for nearly
10 years. The implied rate, which is backed out from the forward points,
looks more realistic, but it can be messy.

Australia and Norway can be backfilled with the 1M swap data. More
worrisome is the choppiness in the forward points (and hence the implied
rate) for CHF, CZK, and SEK.

\subsubsection{Spot Rates}\label{spot-rates}

Spot rates, at least should be freely available. However, once again,
data from Yahoo turned out to be poor enough that proprietary Bloomberg
data was needed. Supposedly weekly data from Yahoo included some data
marked from all days of the week, although not all with the same
frequency.

\subsubsection{Equity Indexes}\label{equity-indexes}

These are indicators of overall stock market performance, for example
the Dow and the S\&P 500 indexes in the US. Data for equity indexes on
Quandl was not up to date. Yahoo had most of it.

Data was patchy for CHF, NZD, and TRY, and non-existant for CZK and NOK.
CZK and NOK were filled in using data downloaded directly from their
respective Bourse's websites.

Finally, there were some timing issues with antipodal stock exchanges.
Yahoo provided all data in the US Eastern time zone, so that AUD and NZD
exchanges open on Sunday night instead of Monday morning. Since the plan
is to use weekly signals, this data needed to be put back in local
timezones.

\subsection{Factors}\label{factors}

The raw data gathered above needed to be transformed into exogenous
variables, typically known as `factors' in finance, but often as
`features' in machine learning parlance. Again, due to space
constraints, the full backfilling and conversion code can be found in
(Naylor 2017), in the
\href{https://charlesnaylor.github.io/gp_regression/doc/Calculating_Factors.html}{Calculating
Factors} section.

\subsubsection{Y variables}\label{y-variables}

The endogenous, `Y' variable comprises weekly currency returns on 11
currencies for a ten-year period starting in 2007 and ending in 2017.

Weekly currency returns consist of the change in spot rate against the
US dollar, plus the carry, defined as the (time-adjusted) difference in
1 month forward rates between the local currency and the USD.

It's important to note that the endogenous variable is
\emph{multivariate}. One cannot assume that different currencies'
returns are independent of one another.

\subsubsection{X variables}\label{x-variables}

The exogenous, `X' variables consist of weekly data for the following:

\begin{longtable}[]{@{}ll@{}}
\toprule
Factor & Description\tabularnewline
\midrule
\endhead
Equity\_d8W & The 8 week change in the local equity index\tabularnewline
Spot\_d8W & The 8 week change in spot rates\tabularnewline
SwapRate2Y & The 2Y Swap Rate\tabularnewline
TwoYear\_d8W & The 8 week change in 2Y Swap rate\tabularnewline
YieldCurve & The spread between 10Y and 2Y Swap rates\tabularnewline
\bottomrule
\end{longtable}

A great deal of currency appreciation is caused by changes in
expectations of interest rates. Hence three of the five factors being
used measure some aspect of those expectations. The change in Equity
indexes is meant to proxy overall economic health. The change in prior
spot rate is a `momentum' indicator, a crude attempt to capture the
chaotic dynamics of competitive agents in an iterative system.

There is clear room for improvement if we were to fit a model that
permitted multiple time scales, such as a MIDAS regression. It would
also be usual to include some sort of measure of liquidity and credit
conditions.

These variables have been scaled and normalized.

Note that Carry is not a stochastic variable. It is, however, a rate,
and impacts asset returns consistently at that rate. It's also worth
noting the degree to which rate differentials have vanished since the
Credit Crisis of 2008.

\section{Models}\label{models}

\subsection{Classification}\label{classification}

\subsubsection{Introduction: Regime Switching
Models}\label{introduction-regime-switching-models}

There has been a substantial amount of work done in economic forecasting
in which time periods are split up into \emph{regimes}, periods in which
markets are expected to behave similarly given similar data. For
example, the markets in the run up to the bursting of the tech bubble in
2000, or the housing credit bubble in 2008, behaved substantially
differently to the markets just after those bubbles. The Kalman Filter
and Gaussian Process regression models attempt to account for changing
reactions to market conditions by fitting a continuously changing
regression to those conditions.

One alternative is a \textbf{Regime Switching Model}. Split the time
periods into several regimes, then run a separate regression for each
regime. The process is complicated by the fact what it can be extremely
difficult to tell at any given moment in which regime markets were
operating at any given time, even in hindsight.

One can attempt to determine these regimes by running various
classifiers and clustering algorithms against the exogenous and
endogenous data.

\subsubsection{Dissimilarity Matrixes}\label{dissimilarity-matrixes}

First it is necessary to compute a dissimilarity matrix between periods.
This calculates the distance between all of the values we have for each
week.

While fitting a global set of regimes, will give a more comprehensible
overview of the techniques, one set of regimes per country will be more
useful for forecasting, as the primary interest is in what happens when
there are differences between countries.

\paragraph{Global Regimes}\label{global-regimes}

Assume the market reactions to economic conditions are the same across
all countries.

\paragraph{Principal Component
Analysis}\label{principal-component-analysis}

It will be hard to see what any of these clusters looks like, as the
feature space is high-dimensional. First, groupings of weeks were
examined using the first two principal components. These might form the
basis of later visualizations.

There is not much evidence of clear regime clusters here.

\paragraph{Per-Country Regimes}\label{per-country-regimes}

How similar are the regimes between countries? There may be an
identification issue as there's no guarantee that, e.g.~regime 1 in one
country will be encoded as regime 1 in another, even if they refer to
similar underlying clusters.

\subsubsection{Hierarchical Clustering}\label{hierarchical-clustering}

In hierarchical clustering, the observations are grouped by similarity
into progressively larger sections, creating a branching set of
categories. Given that the observer should expect relatively few broad
regimes of behavior in total, it will be necessary to cut these branches
early. Probably there should only be 2 or 3 regimes in total, but for
the purposes of validation, the cut was made at 5.

The identified cluster for each week can be seen below:

There are a lot of contiguous blocks, so the technique has clearly
identified something.

\paragraph{Per-country}\label{per-country}

How well does the technique work when applied to individual countries?

There is still quite a lot of commonality between regimes. The least
similar currency is the Turkish Lira, on the bottom row. The British
Pound (middle row) enters a new regime 3 weeks \emph{before} the vote on
exiting the EU, in week 25 of 2016, which is evidence against the
validity of this technique.

\subsubsection{K-means Clustering}\label{k-means-clustering}

Unlike hierarchical clustering, K-means clustering looks at the data
holistically, and attempts to create groups of points around an
arbitrary number of centers. 5 centers were chosen to match the regimes
identified earlier.

Although the Regime numbers are different, the pattern looks fairly
similar to the global regimes made using hierarchical clustering.

\section{Forecasting}\label{forecasting}

As with most other asset classes, currency returns have fat tails, and
the majority of an investor's profit (or loss) is typically made in a
small number of periods. Suprise economic news or geopolitical events
such as the British vote to leave the EU can cause the markets to
reassess appropriate price levels drastically. As a result, a person
discretizing returns so that a classifier can be applied directly would
be well-advised to distinguish between large moves and smaller ones. In
this analysis, discretization of the endogenous variable will be avoided
as much as possible.

One must also be careful when applying typical validation techniques,
particularly cross-validation, to time series. As the clustering
algorithms above clearly demonstrated, one cannot take samples of the
data that assume values will be independent of one another across time.
The ideal equivalent to leave-one-out cross-validation is simply to run
the algorithm every week with the data available for that week, keeping
a single period forecast each time. For the purposes of regressions
conducted with Support Vector Machines and Random Forests, however, the
data will be sliced into training and testing periods midway through the
time series.

To make things convenient, move the data out of tidy format, and match
up next week's returns to this week's forecasting data:

\subsubsection{Support Vector Machines}\label{support-vector-machines}

Although Support Vector Machines (SVMs) are traditionally used for
classification, the technique can also be applied to regression.

The exogenous variables have already been standardized, so a SVM should
fit reasonably well out of the box. As we are forecasting at this point,
the endogenous variables (i.e.~the asset returns) will need to be lagged
by a week. Finally, there is the question of whether all currencies
should be expected to behave similarly given the same economic data. For
now, it will be assumed this is so.

Try an initial run without tuning:

2500 support vectors suggests the regression may not be robust. How well
does it forecast returns for the remaining period?

It remains to be seen how good that SSR score is. Practitioners
typically have low expectations for goodness of fit and rely on various
performance metrics after applying an optimizer to the asset forecasts
to determine realistic investment decisions that could be based on the
results. Asset optimization is out of scope here, however.

\paragraph{Tuning the SVM}\label{tuning-the-svm}

The \texttt{e1071} package can perform a grid search on available
parameters and tune itself, however, the algorithm assumes that
cross-validation is an acceptable techniqe. The SVM will have to be
tuned manually.

What about a more parsimonious representation? The SVM algorithm permits
a regularization parameter, \texttt{cost}, which will penalize overly
complex models. It would also be sensible to try a polynomial kernel, as
asset returns, even normalized, are not expected to be linear with
respect to the economic data. Finally, it is likely this regression will
be noisy, so \(\gamma\) should be set low to reflect the high variance
in the data.

The cost parameter actually increased the number of support vectors
found.

Fit a polynomial kernel:

This one failed to converge, so higher orders of polynomial are right
out.

Finally, set gamma low:

Compare the four models, using Mean Squared Error this time:

The model assuming higher noise did the best. Something very strange is
going on the the high-cost `simple' model, which wound up less simple
than the base.

It's notable for all of these that the number of support vectors is
close to the number of observations (around 90\%), and that high
penalties are not helping the model find an easier solution. SVMs are
probably the wrong approach for forecasts this noisy.

\subsection{Random Forests}\label{random-forests}

Decision Trees (or Regression Trees, in the continuous case) divide
observations into sections according to their similarity, and relate
those sections to the endogenous variable. They are notoriously
unstable, however: a small change in data can result in a completely
different model. Random Forests attempt to address this instability by
taking random samples of the data and fitting separate sets of
Regression Trees to each set, then combining the results. In their raw
form, they must be used with caution as the random sampling will assume
observations are independent, when in fact they are correlated over
time. The \texttt{randomForest} function in R can also be set to return
a proximity measure, similar to the dissimilarity matrixes calculated
above for the classifiers. More on this later.

There is not much gain from fitting more than about 25 trees.

As before, there should be relatively few nodes in the tree, reflecting
relatively few possible macroeconomic regimes for economies. How well
does the Random Forest regression do when \texttt{maxnodes} has been set
quite low?

That seems to have done just as well. Update the Mean Squared Error
table:

Fewer nodes made for a more robust model, but it is only marginally
better than the noise-tolerant SVM.

\subsection{Analogy Weighting}\label{analogy-weighting}

Jim Savage, currently chief Data Scientist at Lendable, Inc., invented
the technique of Analogy Weighting for his PhD thesis. His code is
available \href{https://github.com/khakieconomics/Thesis_work}{here}.
One runs a weighted linear regression on time series data using the
proximity matrix from a Random Forest regression for the weights. The
intuition is that similar periods in history ought to elicit similar
reactions to the data. Dr.~Savage used the data to forecast three
correlated economic time series. A key advantage of the analogy
weighting methodology is that the weights can be set such that the
regression is run on each data point without looking into the future.
This mimics the Kalman Filter workflow in that an investor need not fit
the entire model over again for each new data point. Alternately, one
can see an implementation of a Gaussian Process Regression on this same
data \href{https://charlesnaylor.github.io/gp_regression/}{here}. It was
necessary to fit the model over 500 times, which took considerable
processing power.

For an additional challenge, this forecast will take into account the
differences between countries, while fitting a single set of betas to
all. Outcomes will be modeled as arising from a multivariate normal
process to mimic the manner in which different countries' currencies
affect one another.

Weightings will probably need to be normalized.

\subsubsection{Panel estimation of weighted
regressions}\label{panel-estimation-of-weighted-regressions}

The estimation of a multivariate endogenous variable is complex and best
expressed probabilistically using a generative model. Properly, fake
data should be generated with known parameters, and the model validated
by showing that it can recover those parameters. An example of the
validation workflow can be found can be found
\href{https://charlesnaylor.github.io/gp_regression/doc/Specifying_the_Model-Full_Model.html}{here}
for the Gaussian Process regression mentioned earlier.

The model will be fit using Stan, the current state of the art for
Bayesian generative models. Stan compiles directly to C for efficiency's
sake, so the code in R will be specified as a long string. Best practice
with Stan is to fit using Hamiltonian Monte Carlo, but due to time
constraints this project may settle for maximum likelihood optimization.

As a final input, the estimated covariance of the various currency
returns will be specified directly, for parsimony's sake. A production
system would typically estimate the joint volatility using a GARCH
model.

Prepare the data in the format Stan expects. This was defined in the
\texttt{data} section above.

\paragraph{\texorpdfstring{\(\Sigma_y\)}{\textbackslash{}Sigma\_y}}\label{sigma_y}

Create exponentially weighted variance-covariance matrix. GARCH would be
better, but from prior experience makes little difference to weekly
currency returns.

Prepare the rest of the data

Run direct maximum likelihood optimization instead of Hamiltonian Monte
Carlo due to time constraints.

Extract parameters

The betas could be artificially induced to be more stable, as in a State
Space Model. The fact that they are not naturally so, and have such
small values, indicates that most of the variation in asset returns
cannot be explained by the weighted model.

How well did the model predict returns?

On its own, the analogy-weighted model is one of the poorest options.

\subsection{Taming the Chaos}\label{taming-the-chaos}

The introduction mentioned the chaotic nature of currency returns.
Recently, a team at the University of Maryland have had some success in
forecasting chaotic time series several steps ahead (Pathak et al.
2018). They took a deterministic chaotic system known as the
Kuramoto-Sivashinksy equation, and fed measurements into a set of 64
Echo State Network layers, with each layer handling a smaller section of
the overall data. An Echo State Network layer consists of a relatively
large number of nodes with sparse, random connections (essentially an
Erdos-Renyi network rather than the scale-free network believed to be
the actual characterization of currency returns) (Lukoševičius 2012).
The input for this implementation will consist of the residuals from the
Analogy-weighted regression computed before.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-barabasi2002linked}{}
Barabasi, Albert-Laszlo. 2002. ``Linked the New Science of Networks.''
Cambridge, Mass.: Perseus Pub.
\url{http://www.amazon.com/Linked-The-New-Science-Networks/dp/0738206679}.

\hypertarget{ref-Luko2012}{}
Lukoševičius, Mantas. 2012. ``A Practical Guide to Applying Echo State
Networks.'' In \emph{Neural Networks: Tricks of the Trade: Second
Edition}, edited by Grégoire Montavon, Geneviève B. Orr, and
Klaus-Robert Müller, 659--86. Berlin, Heidelberg: Springer Berlin
Heidelberg.
doi:\href{https://doi.org/10.1007/978-3-642-35289-8_36}{10.1007/978-3-642-35289-8\_36}.

\hypertarget{ref-naylor_GP}{}
Naylor, Charles. 2017. ``Gaussian Process Regression for Fx Forecasting:
A Case Study.'' \url{https://charlesnaylor.github.io/gp_regression/}.

\hypertarget{ref-PhysRevLett.120.024102}{}
Pathak, Jaideep, Brian Hunt, Michelle Girvan, Zhixin Lu, and Edward Ott.
2018. ``Model-Free Prediction of Large Spatiotemporally Chaotic Systems
from Data: A Reservoir Computing Approach.'' \emph{Phys. Rev. Lett.} 120
(2). American Physical Society: 024102.
doi:\href{https://doi.org/10.1103/PhysRevLett.120.024102}{10.1103/PhysRevLett.120.024102}.


\end{document}
